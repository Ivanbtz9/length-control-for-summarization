{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Bart large on CNN daily news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES ###\n",
    "\n",
    "import sys,os\n",
    "import tqdm\n",
    "import csv\n",
    "from datetime import datetime \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  12\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "NUM_LOADER = 4 #config['config_machine'][\"NUM_LOADER\"] #depends of the number of thread \n",
    "\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset CNN daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 14355\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 668\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 574\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "## Comment this part for the real training time :\n",
    "\n",
    "percentage = 0.05\n",
    "\n",
    "for split in dataset: \n",
    "    dataset[split] = dataset[split].shuffle(seed=SEED).select(range(int(len(dataset[split]) * percentage)))\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model and tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n",
      "BartTokenizerFast(name_or_path='facebook/bart-large', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### Load model ###\n",
    "MODEL_HUB = 'facebook/bart-large'\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HUB, clean_up_tokenization_spaces=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_HUB, forced_bos_token_id=0)\n",
    "print(tokenizer.model_max_length)\n",
    "print(type(tokenizer))\n",
    "print(type(model))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a74316fc84102bdd092b5dd5ca3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/14355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1498 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1585 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1888 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1717 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1142 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1702 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6267a8aba947c4807535e69e11ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1752 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1689 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1580 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1466 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1953 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e0aca2f476476398c35b92d94006c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1647 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1917 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1592 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1529 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2226 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=tokenizer.model_max_length,padding='max_length')\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=tokenizer.model_max_length,padding='max_length')\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=64)# Save the Hugging Face dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2765,   479,  ...,     1,     1,     1],\n",
      "        [    0,   250,  3828,  ...,     5,  1151,     2],\n",
      "        [    0,   250, 17052,  ...,     1,     1,     1],\n",
      "        [    0,   970,    58,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_input_ids': tensor([[    0,  9058,  2152,  ...,     1,     1,     1],\n",
      "        [    0,   104,  4774,  ...,     1,     1,     1],\n",
      "        [    0,   250,  1150,  ...,     1,     1,     1],\n",
      "        [    0, 14563,  7414,  ...,     1,     1,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'input_len': tensor([ 331, 1393,  727,  498]), 'target_len': tensor([ 53, 100,  97,  58])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    input_ids = torch.tensor([item['input_ids'] for item in batch], dtype=torch.long)\n",
    "    # input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    attention_mask = torch.tensor([item['input_mask']for item in batch], dtype=torch.long) \n",
    "    # attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    decoder_input_ids  = torch.tensor([item['target_ids'] for item in batch], dtype=torch.long) #item['target_ids'][:-1]\n",
    "    # decoder_input_ids = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = torch.tensor([item['target_mask'] for item in batch], dtype=torch.long)#item['target_mask'][:-1]\n",
    "    # decoder_attention_mask = pad_sequence(decoder_attention_mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    input_len = torch.tensor([item['input_len'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    target_len = torch.tensor([item['target_len'] for item in batch], dtype=torch.long)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids':input_ids,\n",
    "        'attention_mask':attention_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'decoder_attention_mask':decoder_attention_mask,\n",
    "        'input_len': input_len,\n",
    "        'target_len': target_len\n",
    "    }\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 4,\n",
    "    'shuffle': True,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "eval_params = {\n",
    "    'batch_size': 4,\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "train_loader = DataLoader(dataset[\"train\"], **train_params)\n",
    "eval_loader = DataLoader(dataset[\"validation\"], **eval_params)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulation on the different part of the Bart model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>, <class 'transformers.models.bart.modeling_bart.BartPreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all parent classes in the MRO (Method Resolution Order)\n",
    "print(BartForConditionalGeneration.__mro__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivanhoe/Bureau/length-control-for-summarization/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test de génération from huggingface :\n",
    "\n",
    "tokenizer.batch_decode(model.generate(**tokenizer([\"UN Chief Says There Is No <mask> in Syria\"], return_tensors=\"pt\")),skip_special_tokens=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 53.,  52.,  51.,  ...,   0.,   0.,   0.],\n",
      "        [100.,  99.,  98.,  ...,   0.,   0.,   0.],\n",
      "        [ 97.,  96.,  95.,  ...,   0.,   0.,   0.],\n",
      "        [ 58.,  57.,  56.,  ...,   0.,   0.,   0.]])\n",
      "tensor([[ 52,  51,  51,  ...,   0,   0,   0],\n",
      "        [ 98, 100,  99,  ...,   0,   0,   0],\n",
      "        [ 97,  96,  96,  ...,   0,   0,   0],\n",
      "        [ 57,  56,  57,  ...,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "mask = ~torch.isin(batch[\"decoder_input_ids\"],torch.tensor([tokenizer.pad_token_id])) # mask with 0 where a pad_id is present\n",
    "\n",
    "reversed_position_input  = torch.ones(mask.shape) * mask # [1,1,1,0,0] \n",
    "\n",
    "reversed_position_input = torch.flip(torch.flip(reversed_position_input , dims=(1,)).cumsum(dim=1), dims=(1,))  \n",
    "\n",
    "print(reversed_position_input )\n",
    "\n",
    "normal_round = torch.randn(batch[\"decoder_input_ids\"].shape) * mask\n",
    "\n",
    "reversed_position_input = torch.abs(torch.round(reversed_position_input  + normal_round)).to(torch.long) #add a gausian noise and converte to long\n",
    "print(reversed_position_input)\n",
    "\n",
    "# input_decoder_position_embedding = model.model.decoder.embed_positions(reversed_position_input)\n",
    "\n",
    "# input_decoder_position_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartDecoder(\n",
      "  (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "  (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x BartDecoderLayer(\n",
      "      (self_attn): BartSdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartSdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.decoder)\n",
    "token_embeddings = model.model.decoder.embed_tokens(batch[\"decoder_input_ids\"]) \n",
    "position_embeddings = model.model.decoder.embed_positions(batch[\"decoder_input_ids\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 9.8217e-01,  ..., 1.0366e-04, 1.0182e-04,\n",
      "         1.0182e-04],\n",
      "        [2.0000e+00, 2.0000e+00, 1.9643e+00,  ..., 2.0733e-04, 2.0363e-04,\n",
      "         2.0363e-04],\n",
      "        ...,\n",
      "        [1.0210e+03, 1.0210e+03, 1.0028e+03,  ..., 1.0584e-01, 1.0395e-01,\n",
      "         1.0395e-01],\n",
      "        [1.0220e+03, 1.0220e+03, 1.0038e+03,  ..., 1.0594e-01, 1.0406e-01,\n",
      "         1.0406e-01],\n",
      "        [1.0230e+03, 1.0230e+03, 1.0048e+03,  ..., 1.0605e-01, 1.0416e-01,\n",
      "         1.0416e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9866, -0.1630,  0.7225,  ...,  1.0000,  0.0053,  1.0000],\n",
       "         [ 0.6702,  0.7422, -0.1738,  ...,  1.0000,  0.0052,  1.0000],\n",
       "         [ 0.6702,  0.7422, -0.1738,  ...,  1.0000,  0.0052,  1.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[-0.5734, -0.8193,  0.9072,  ...,  0.9999,  0.0100,  1.0000],\n",
       "         [-0.5064,  0.8623, -0.7365,  ...,  0.9999,  0.0102,  0.9999],\n",
       "         [-0.9992,  0.0398,  0.1537,  ...,  0.9999,  0.0101,  0.9999],\n",
       "         ...,\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.3796, -0.9251,  0.8536,  ...,  0.9999,  0.0099,  1.0000],\n",
       "         [ 0.9836, -0.1804,  0.0407,  ...,  1.0000,  0.0098,  1.0000],\n",
       "         [ 0.9836, -0.1804,  0.0407,  ...,  1.0000,  0.0098,  1.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.4362,  0.8999, -0.5353,  ...,  1.0000,  0.0058,  1.0000],\n",
       "         [-0.5216,  0.8532, -0.9997,  ...,  1.0000,  0.0057,  1.0000],\n",
       "         [ 0.4362,  0.8999, -0.5353,  ...,  1.0000,  0.0058,  1.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # dir(model.model.decoder.layernorm_embedding)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(model.model.decoder.layernorm_embedding.weight.detach().numpy())\n",
    "\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "d_model = 1024\n",
    "max_len = tokenizer.model_max_length\n",
    "\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "div_term = torch.exp(-2*(torch.arange(0, d_model)//2) / d_model * math.log(10000.0) )\n",
    "\n",
    "print(div_term.shape)\n",
    "\n",
    "print(position* div_term)\n",
    "\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position*div_term[0::2] )\n",
    "pe[:, 1::2] = torch.cos(position*div_term[1::2])\n",
    "\n",
    "embed_reverse_positions = nn.Embedding(num_embeddings=tokenizer.model_max_length,\n",
    "                                              embedding_dim=d_model,\n",
    "                                              padding_idx=tokenizer.pad_token_id,\n",
    "                                              _weight=pe,\n",
    "                                              _freeze=True)\n",
    "\n",
    "embed_reverse_positions(reversed_position_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
