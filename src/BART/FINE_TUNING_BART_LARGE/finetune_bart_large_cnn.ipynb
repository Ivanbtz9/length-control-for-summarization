{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Bart large on CNN daily news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES ###\n",
    "\n",
    "import sys,os\n",
    "import tqdm\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "\n",
    "#link : https://github.com/facebookresearch/fairseq/tree/main/examples/bart\n",
    "#link : https://huggingface.co/docs/transformers/model_doc/bart\n",
    "#link : https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/model#transformers.PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  12\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_machine': {'SEED': 42, 'NUM_LOADER': 50}, 'config_model': {'MODEL_TYPE': 'bart', 'MODEL_NAME': 'bart-large', 'MODEL_HUB': 'facebook/bart-large'}, 'config_generate': {'NUM_BEAM': 4, 'min_len_resume': 14, 'max_len_resume': 159, 'no_repeat_ngram_size': 3, 'repetition_penalty': 1.5, 'length_penalty': 2, 'early_stopping': True, 'use_cache': False}, 'config_training': {'max_len': 1024, 'TRAIN_BATCH_SIZE': 16, 'VALID_BATCH_SIZE': 32, 'LEARNING_RATE': 0.0001, 'NB_EPOCHS': 15, 'early_stopping_patience': 5, 'reduce_lr_patience': 2, 'reduce_lr_factor': 0.1}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from JSON\n",
    "with open('./config_finetune_bart_large.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "    print(type(config))\n",
    "\n",
    "SEED = config['config_machine'][\"SEED\"]\n",
    "NUM_LOADER = 4 #config['config_machine'][\"NUM_LOADER\"] #depends of the number of thread \n",
    "\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset CNN daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 14355\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 668\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 574\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "## Comment this part for the real training time :\n",
    "\n",
    "percentage = 0.05\n",
    "\n",
    "for split in dataset: \n",
    "    dataset[split] = dataset[split].shuffle(seed=SEED).select(range(int(len(dataset[split]) * percentage)))\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model and tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "### Load model ###\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "MODEL_HUB = config[\"config_model\"][\"MODEL_HUB\"]\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HUB, clean_up_tokenization_spaces=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_HUB, forced_bos_token_id=0)\n",
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f338e3dffa41b5b9765c3c77e3911c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/14355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1142 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1498 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1717 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1585 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1702 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1888 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926c7e960155486591ce33988fff8576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1752 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1580 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1689 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1466 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1953 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c88094e4658467f82f889440698e061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1647 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1592 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1917 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1529 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2226 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=tokenizer.model_max_length)\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=tokenizer.model_max_length)\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=64)# Save the Hugging Face dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['08cf276c9eadb638e0c7fdc83ce0229c8af5d09b', 'a0965f34cb08bd7db5845f8285dc8a9512d3e590', 'e960738caa9d39737f459e39923b20160f73f258', '9befc7ac904202d6e018093c2923f04479d6af05', 'dc3c9f319ca17b33ad87c71049a8589e474ba769', 'a6c74feb871eb462a645b442d23afb0098fd304e', '6182e63f54f7d4fc1e03caeb752d69e784c531a4', 'b2d77b13bd8aaf7ae9ba8f1e2771e115dd665698', '4bb5657c537eb9f8d00876985b91c578ab1ec3d8', '40d500f482a41b209ff379f2207180b71dd3bf35', '557fd3f6fc104f27dc6071bd80dc7c0b2c19cafa', '0bb9c54124e50dabcdc9c44828470b30024657b0', '4a0caecbc2bdd104e56828bc68a8244f1b3743f6', '427816b50f73c14cd2b6ea9dc088ddeb4e7f72a3', 'bc98b9b915afcc5a9fe1d1fb868ee9b546cfda06', 'ccc3717dca507094ffba8a59d8185bd6b0ad8d4a'], 'input_ids': tensor([[    0,  2765,   479,  ...,     1,     1,     1],\n",
      "        [    0,  4154, 33957,  ..., 13223,  2637,     2],\n",
      "        [    0, 45158,    12,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  1640, 16256,  ...,     1,     1,     1],\n",
      "        [    0, 24485,  1422,  ...,     1,     1,     1],\n",
      "        [    0,  2765,   479,  ...,   173,    14,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0, 10567,     8,  ...,     1,     1,     1],\n",
      "        [    0,  5341,    35,  ...,     1,     1,     1],\n",
      "        [    0, 25101, 29622,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 15638,  6518,  ...,     1,     1,     1],\n",
      "        [    0,   500, 11688,  ...,     1,     1,     1],\n",
      "        [    0, 18551,  2126,  ...,     1,     1,     1]]), 'target_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'input_len': [663, 1105, 741, 1368, 622, 803, 445, 608, 632, 443, 620, 496, 688, 377, 716, 1054], 'target_len': [47, 68, 66, 45, 53, 111, 47, 47, 89, 30, 51, 100, 55, 61, 61, 29], 'highlights': ['John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'NEW: Libya can serve as example of cooperation, White House spokesman says .\\nResolution calls for preventing nuclear weapons from being stolen, used by military .\\nObama, Russian President Dimitry Medvedev working to reduce stockpiles .\\nVenezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .', 'Very Reverend Robert Waddington sexually abused choirboys for decades .\\nInquiry into the abuse has slammed\\xa0Lord Hope, former Archbishop of York .\\nReport claims he was made aware of misconduct 19 times but did not act .\\nDespite this - he still holds influential post and sits in the House of Lords .', \"Monday night's episode showed Buddy Valastro tricking Anthony Bellifemine into thinking that Carmen Carerra, 27, was born as a woman .\\nTLC removed the episode from future screening schedules .\", 'People asked to turn out lights for hour between 10 and 11pm tomorrow .\\nGesture is in\\xa0remembrance of those killed in the First World War .\\nTower Bridge and 10 Downing Street will also extinguish all but one light .', \"Roy Hodgson revealed that Raheem Sterling had told him he was feeling fatigued prior to England's Euro 2016 qualifier with Estonia .\\nSterling was dropped from the starting line-up in place of Adam Lallana .\\nThe Liverpool man later came on as a substitute as England won 1-0 .\\nHodgson was criticised by Sky Sports pundits Jamie Redknapp and Jamie Carragher for making Sterling's admission of tiredness public .\\nWayne Rooney's free-kick preserved England's perfect start to qualifying .\", 'Calls for every police officer to be offered a Taser to fight terrorist threat .\\nPolice Federation set to vote on giving all frontline officers training .\\nTerrorist threat level for police was raised to severe after Paris attacks .', \"800 common pavement ants are now living on International Space Station .\\nScientists will examine how they work together in low gravity to find food .\\nThe ants' methods can then be copied to develop 'intelligent' search robots .\", 'David Wilcockson, 71, was bowling at a ground in  Cranleigh, Surrey when the ball struck him on the head .\\nDied in hospital on June 1 after 13 days in a coma .\\nHe was the longest-serving member of the Old Dorkinians, joining the club in 1959 .\\nThe pensioner had set himself a target of 3,000 wickets - and died just 101 short .', 'The boulder was found by a construction crew in Everett, Washington .\\nThe rock is bigger than an SUV and close to 19 feet long .', 'Abby Miller sings, plays guitar to help 4-year-old friend with cancer .\\nAbby collects donations, has people write notes of support for Taylor Love .\\nTaylor has neuroblastoma, a cancer that affects the nervous system .', \"Denise Shepherd died from an overdose of heroin and a cocktail of pills .\\nFound dead in flat she shared with her boyfriend in Eastbourne last year .\\nInquest into her death heard she became addicted to drugs at the age of 11 .\\nHearing told she smoked cannabis and the progressed to harder drugs .\\nHer mother Joann Bell says her daughter 'fell in with the wrong crowd'\\nCoroner Alan Craze recorded a verdict of death due to a dependence on drugs .\", 'Corning unveils a new, thinner form of its popular Gorilla Glass at CES .\\nThe original Gorilla Glass is used in more than 500 models of touchscreen gadgets .\\nNew glass more responsive but not less resistant to breakage than current Gorilla Glass .', 'Red Bull Formula One team \"devastated\" by burglary .\\nThieves drove away with more than 60 trophies after Friday night break in .\\nThames Valley Police say six men and two cars used in robbery .\\nRed Bull team boss Christian Horner says value of trophies is low .', \"Rapper Rick Ross and a female passenger were not hurt by the wreck or gunshots .\\nA street gang posted online death threats against Ross in recent months .\\nRoss, 37, says he's not intimidated by the threats .\\nA former drug lord accuses Ross of stealing his name and reputation .\", 'Colourful new range launches May 14 .\\nHas expanded to include eye collection .\\nOnly unveiled Topshop collection two days ago .']}\n"
     ]
    }
   ],
   "source": [
    "## TO CONTINUE\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    id = [item['id'] for item in batch]\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    padded_text_ids = pad_sequence(\n",
    "        [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    padded_text_mask = pad_sequence(\n",
    "        [torch.tensor(item['input_mask'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=0)\n",
    "\n",
    "    decoder_input_ids = pad_sequence(\n",
    "        [torch.tensor(item['target_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = pad_sequence(\n",
    "        [torch.tensor(item['target_mask'], \n",
    "                      dtype=torch.long) for item in batch], \n",
    "                      batch_first=True, \n",
    "                      padding_value=0)\n",
    "    \n",
    "    input_len = [item['input_len'] for item in batch]\n",
    "\n",
    "    target_len = [item['target_len'] for item in batch]\n",
    "\n",
    "    highlights = [item['highlights'] for item in batch] \n",
    "    \n",
    "\n",
    "    return {\n",
    "        'id':id,\n",
    "        'input_ids':padded_text_ids,\n",
    "        'attention_mask':padded_text_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'target_mask':decoder_attention_mask,\n",
    "        'input_len': input_len ,\n",
    "        'target_len': target_len,\n",
    "        'highlights': highlights\n",
    "    }\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': config[\"config_training\"][\"TRAIN_BATCH_SIZE\"],\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "train_loader = DataLoader(dataset[\"train\"], **train_params)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
