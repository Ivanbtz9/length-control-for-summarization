{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Bart large on CNN daily news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES ###\n",
    "\n",
    "import sys,os\n",
    "import tqdm\n",
    "import csv\n",
    "from datetime import datetime \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "\n",
    "#link : https://github.com/facebookresearch/fairseq/tree/main/examples/bart\n",
    "#link : https://huggingface.co/docs/transformers/model_doc/bart\n",
    "#link : https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/model#transformers.PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  12\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_machine': {'SEED': 42, 'NUM_LOADER': 50}, 'config_model': {'MODEL_TYPE': 'bart', 'MODEL_NAME': 'bart-large', 'MODEL_HUB': 'facebook/bart-large'}, 'config_generate': {'num_beams': 4, 'min_length': 14, 'max_length': 200, 'no_repeat_ngram_size': 3, 'repetition_penalty': 1.5, 'length_penalty': 2, 'early_stopping': True, 'do_sample': True}, 'config_training': {'max_len': 1024, 'TRAIN_BATCH_SIZE': 2, 'VALID_BATCH_SIZE': 4, 'LEARNING_RATE': 0.0001, 'NB_EPOCHS': 15, 'early_stopping_patience': 5, 'reduce_lr_patience': 2, 'reduce_lr_factor': 0.1}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from JSON\n",
    "with open('./config_finetune_bart_large.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "    print(type(config))\n",
    "\n",
    "SEED = config['config_machine'][\"SEED\"]\n",
    "NUM_LOADER = 4 #config['config_machine'][\"NUM_LOADER\"] #depends of the number of thread \n",
    "\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset CNN daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 14355\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 668\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 574\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "## Comment this part for the real training time :\n",
    "\n",
    "percentage = 0.05\n",
    "\n",
    "for split in dataset: \n",
    "    dataset[split] = dataset[split].shuffle(seed=SEED).select(range(int(len(dataset[split]) * percentage)))\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model and tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "<class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "### Load model ###\n",
    "MODEL_HUB = config[\"config_model\"][\"MODEL_HUB\"]\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HUB, clean_up_tokenization_spaces=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_HUB, forced_bos_token_id=0)\n",
    "print(tokenizer.model_max_length)\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=tokenizer.model_max_length)\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=tokenizer.model_max_length)\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=64)# Save the Hugging Face dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  4030,  3534,  ...,     1,     1,     1],\n",
      "        [    0,  1640, 16256,  ...,     5, 23808,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0,   448,  3853,   661,  7369, 30421,   877,    10,  1212,  4318,\n",
      "             6,   490,   668,    15,    10,   559,  4243, 26671,   479, 50118,\n",
      "         36342,     5,   706,   848,    16,     5,  3787,     9,    41,  1475,\n",
      "            12,   448,  3853,   661, 11941, 17157,     6,   799,   249,  1024,\n",
      "           479, 50118,  2895,    82,    11,     5, 22369,    58,    31,   666,\n",
      "            18,  2255,  1148,   537,   479, 50118, 25973,   692,  1554,   119,\n",
      "         14669,  3657,  5695,     5,  1710,     6, 23297,     7,   465, 17685,\n",
      "            29,   479],\n",
      "        [    0,   250,    92,   266,   924,    14, 37442,  2099,   227,   604,\n",
      "            16,    15,     5,  1430,   479, 50118,  6323,    33,  8113,   119,\n",
      "          1720,  3240,  5100,   604,  2025,    75,  2273,    59,  7947,   479,\n",
      "         50118,  4688,    22,  2013, 37465, 11079,   113,   227,  6808,   531,\n",
      "            28, 30987,  4462,     6,  6220,   289,  9707, 10100,   161,   479,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  448,  3853,   661,  7369, 30421,   877,    10,  1212,  4318,     6,\n",
      "           490,   668,    15,    10,   559,  4243, 26671,   479, 50118, 36342,\n",
      "             5,   706,   848,    16,     5,  3787,     9,    41,  1475,    12,\n",
      "           448,  3853,   661, 11941, 17157,     6,   799,   249,  1024,   479,\n",
      "         50118,  2895,    82,    11,     5, 22369,    58,    31,   666,    18,\n",
      "          2255,  1148,   537,   479, 50118, 25973,   692,  1554,   119, 14669,\n",
      "          3657,  5695,     5,  1710,     6, 23297,     7,   465, 17685,    29,\n",
      "           479,     2],\n",
      "        [  250,    92,   266,   924,    14, 37442,  2099,   227,   604,    16,\n",
      "            15,     5,  1430,   479, 50118,  6323,    33,  8113,   119,  1720,\n",
      "          3240,  5100,   604,  2025,    75,  2273,    59,  7947,   479, 50118,\n",
      "          4688,    22,  2013, 37465, 11079,   113,   227,  6808,   531,    28,\n",
      "         30987,  4462,     6,  6220,   289,  9707, 10100,   161,   479,     2,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "## TO CONTINUE\n",
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    input_ids = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    attention_mask = [torch.tensor(item['input_mask'], dtype=torch.long) for item in batch]\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    decoder_input_ids  = [torch.tensor(item['target_ids'][:-1], dtype=torch.long) for item in batch]\n",
    "    decoder_input_ids = pad_sequence(decoder_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = [torch.tensor(item['target_mask'][:-1], dtype=torch.long) for item in batch]\n",
    "    decoder_attention_mask = pad_sequence(decoder_attention_mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # input_len = torch.tensor([item['input_len'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    # target_len = torch.tensor([item['target_len'] for item in batch], dtype=torch.long)\n",
    "\n",
    "    # Labels should be the same as decoder_input_ids (BART-style training)\n",
    "    labels = [torch.tensor(item['target_ids'][1:], dtype=torch.long) for item in batch]\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)  \n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in loss computation\n",
    "\n",
    "    return {\n",
    "        'input_ids':input_ids,\n",
    "        'attention_mask':attention_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'decoder_attention_mask':decoder_attention_mask,\n",
    "        'labels': labels,\n",
    "        # 'input_len': input_len,\n",
    "        # 'target_len': target_len\n",
    "    }\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': config[\"config_training\"][\"TRAIN_BATCH_SIZE\"],\n",
    "    'shuffle': True,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "eval_params = {\n",
    "    'batch_size': config[\"config_training\"][\"VALID_BATCH_SIZE\"],\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "train_loader = DataLoader(dataset[\"train\"], **train_params)\n",
    "eval_loader = DataLoader(dataset[\"validation\"], **eval_params)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with torch.no_grad():\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     batch = {key: val.to(device) for key, val in batch.items()}\n",
    "#     output = model.generate(input_ids = batch[\"input_ids\"], attention_mask= batch[\"attention_mask\"], **config['config_generate'])\n",
    "#     decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "#     print(\"Generate summary: \" , decoded_output,\"\\n\\n\")\n",
    "#     print(\"Expected summary: \" , tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement des poids du modèle et sauvegarde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, loader, optimizer, epoch, writer):\n",
    "    \"\"\"\n",
    "    Function to call for training with the parameters passed from main function\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for _, batch in tqdm.tqdm(enumerate(loader, 0),desc=f'Total iterations: {len(loader)}', unit=\" it\"):\n",
    "\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        writer.add_scalar(f\"batch_loss/train\", loss/len(batch[\"input_ids\"]), epoch*len(loader) + _ )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return {\"loss\": total_loss/len(loader)} \n",
    "\n",
    "def eval(model, device, loader, epoch, writer): #epoch,writer \n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for evaluate with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in tqdm.tqdm(enumerate(loader, 0),desc=f'Total iterations: {len(loader)}', unit=\" it\"):\n",
    "\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            writer.add_scalar(f\"batch_loss/train\", loss/len(batch[\"input_ids\"]), epoch*len(loader) + _ )\n",
    "\n",
    "    return {\"loss\": total_loss/len(loader)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "time = datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "\n",
    "checkpoint_path = f\"./models_checkpoints/Bart-large-{time}\"\n",
    "os.makedirs(checkpoint_path,exist_ok=True)\n",
    "\n",
    "log_dir = f\"./logs/Bart-large-{time}\" \n",
    "os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=config['config_training'][\"LEARNING_RATE\"])\n",
    "\n",
    "early_stopping_patience = config['config_training'][\"early_stopping_patience\"]\n",
    "reduce_lr_patience = config['config_training'][\"reduce_lr_patience\"]\n",
    "reduce_lr_factor = config['config_training'][\"reduce_lr_factor\"]\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=reduce_lr_patience, factor=reduce_lr_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total iterations: 167: 167 it [13:26,  4.83s/ it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.0759328146894536}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(model, device, eval_loader, 0, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, device, train_loader, eval_loader, optimizer, writer, scheduler, checkpoint_path, NB_EPOCHS):\n",
    "        \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    current_lr = 0\n",
    "\n",
    "    eval_loss, train_loss = [], []\n",
    "    \n",
    "    for epoch in range(NB_EPOCHS):\n",
    "        outputs_train = train(model, device, train_loader, optimizer, epoch, writer)\n",
    "        outputs_eval = eval(model, device, eval_loader, epoch, writer)\n",
    "        \n",
    "        train_loss.append(outputs_train[\"loss\"])\n",
    "        eval_loss.append(outputs_eval[\"loss\"])\n",
    "\n",
    "        writer.add_scalars(\"loss\",{\"train\":outputs_train[\"loss\"], \"eval\":outputs_eval[\"loss\"] } , epoch)\n",
    "\n",
    "\n",
    "        # Enregistrer le taux d'apprentissage actuel\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "            writer.add_scalar(\"lr\", current_lr, epoch)\n",
    "\n",
    "        # Réduit automatiquement le taux d'apprentissage si la perte de validation ne diminue pas\n",
    "        # sous les critères\n",
    "        scheduler.step(eval_loss[-1])\n",
    "\n",
    "        # Early Stopping\n",
    "        if eval_loss[-1] < best_loss:\n",
    "            best_loss = eval_loss[-1]\n",
    "            patience_counter = 0\n",
    "            # Sauvegarder le meilleur modèle\n",
    "            model.save_pretrained(checkpoint_path)\n",
    "            tokenizer.save_pretrained(checkpoint_path)\n",
    "            epoch_best = epoch+1\n",
    "            print(f'Find a better model at the epoch {epoch+1} - Train Loss: {train_loss[-1]:.4f}, eval Loss: {eval_loss[-1]:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                print(\"current_lr : \", current_lr)\n",
    "                print(\"epoch_best : \", epoch_best)\n",
    "                break\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(model.to(device), device, train_loader, eval_loader, optimizer, writer,scheduler,checkpoint_path, config['config_training'][\"NB_EPOCHS\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
