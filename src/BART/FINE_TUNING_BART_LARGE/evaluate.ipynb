{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Finetuning Bart large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#link : https://github.com/facebookresearch/fairseq/tree/main/examples/bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  12\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "NUM_LOADER = 4 #depends of the number of thread \n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "MODEL_HUB = \"facebook/bart-large\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# max_len = 1024\n",
    "\n",
    "\n",
    "# BATCH_SIZE =2\n",
    "\n",
    "# NUM_BEAM = 5\n",
    "\n",
    "# max_len_resume = 200\n",
    "# repetition_penalty=2.0\n",
    "# length_penalty=1.0\n",
    "# early_stopping=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset CNN daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f600aec17b46838f89ec7031eee743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/28712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc39d15ecb424e6aa0ae3358f8bef2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/28712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    # Prefix the \"summarize: \" instruction to each article (can be adjusted depending on your task)\n",
    "    batch[\"article\"] = [\"summarize: \" + article for article in batch[\"article\"]]\n",
    "\n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=max_len)\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=max_len)\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=32)# Save the Hugging Face dataset\n",
    "dataset.save_to_disk('data/cnn_dailymail')\n",
    "print(\"Dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'input_mask', 'input_len', 'target_ids', 'target_mask', 'target_len'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shuffle(seed=5).select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['08cf276c9eadb638e0c7fdc83ce0229c8af5d09b', 'a0965f34cb08bd7db5845f8285dc8a9512d3e590'], 'input_ids': tensor([[    0, 18581,  3916,  ...,     1,     1,     1],\n",
      "        [    0, 18581,  3916,  ...,   185,   480,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0, 10567,     8,   479, 50118, 37779,  5460,  4350,    58,  2967,\n",
      "          2863,    49,  1354,     6,  3066, 26743,   479, 50118,  1213,    58,\n",
      "           303,    23, 31103,   271,   459,  2193,   861,    11, 21690,   479,\n",
      "         50118, 40333,   224,     5,   130,   962,     9,  4363,  6154, 24260,\n",
      "           479, 50118,  5873,  4060,   154,   479,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  5341,    35,  7662,    64,  1807,    25,  1246,     9,  4601,\n",
      "             6,   735,   446,  1565,   161,   479, 50118, 20028, 23794,  1519,\n",
      "            13,  9107,  1748,  2398,    31,   145,  3579,     6,   341,    30,\n",
      "           831,   479, 50118, 33382,     6,  1083,   270, 11709,   405,  1506,\n",
      "          5066,  5202,  3623,   447,     7,  1888, 21773,  4755,   479, 50118,\n",
      "           846, 41025,   260,   394, 18148, 14916,    15,    22, 38544,  1745,\n",
      "          3561,   113,  3422,     6,   361,  4799,   479,     2]]), 'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_len': [667, 1108], 'target_len': [47, 68], 'highlights': ['John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'NEW: Libya can serve as example of cooperation, White House spokesman says .\\nResolution calls for preventing nuclear weapons from being stolen, used by military .\\nObama, Russian President Dimitry Medvedev working to reduce stockpiles .\\nVenezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    id = [item['id'] for item in batch]\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    padded_text_ids = pad_sequence(\n",
    "        [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    padded_text_mask = pad_sequence(\n",
    "        [torch.tensor(item['input_mask'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=0)\n",
    "\n",
    "    decoder_input_ids = pad_sequence(\n",
    "        [torch.tensor(item['target_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = pad_sequence(\n",
    "        [torch.tensor(item['target_mask'], \n",
    "                      dtype=torch.long) for item in batch], \n",
    "                      batch_first=True, \n",
    "                      padding_value=0)\n",
    "    \n",
    "    input_len = [item['input_len'] for item in batch]\n",
    "\n",
    "    target_len = [item['target_len'] for item in batch]\n",
    "\n",
    "    highlights = [item['highlights'] for item in batch]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return {\n",
    "        'id':id,\n",
    "        'input_ids':padded_text_ids,\n",
    "        'attention_mask':padded_text_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'target_mask':decoder_attention_mask,\n",
    "        'input_len': input_len ,\n",
    "        'target_len': target_len,\n",
    "        'highlights': highlights\n",
    "    }\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "loader = DataLoader(dataset, **params)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7833333333333332, 'rouge2': 0.5833333333333334, 'rougeL': 0.7833333333333332, 'rougeLsum': 0.7833333333333332}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "candidates = [\"Summarization is cool\",\"I love Machine Learning\",\"Good night\"]\n",
    "\n",
    "references = [\"summarization is beneficial and cool\",\"i think i love Machine Learning\",\"Good night everyone!\"]\n",
    "             \n",
    "results = rouge.compute(predictions=candidates, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./rouge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    writer.writerow(field)\n",
    "\n",
    "with open('./len.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"id\", \"input_len\", \"target_len\", \"generate_len\"]\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "total iter: 14356: 1 iter [01:15, 75.20s/ iter]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "rouge1_score, rouge2_score , rougeL_score = 0, 0, 0\n",
    "nb_sample = 0\n",
    "\n",
    "exclude_ids = torch.tensor([0, 1, 2, 3, 50264]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for _, batch in tqdm.tqdm(enumerate(loader, 0),desc=f'total iter: {len(loader)}', unit=\" iter\"):\n",
    "        \n",
    "\n",
    "        generated_ids = model.generate(\n",
    "              input_ids = batch[\"input_ids\"].to(device),\n",
    "              attention_mask = batch[\"attention_mask\"].to(device), \n",
    "              max_length=max_len_resume, \n",
    "              num_beams=NUM_BEAM,\n",
    "              repetition_penalty=repetition_penalty, \n",
    "              length_penalty=length_penalty, \n",
    "              early_stopping=early_stopping\n",
    "              )   \n",
    "        #print(generated_ids)\n",
    "\n",
    "        generated_txt = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        #print(generated_txt)\n",
    "        #print(type(generated_txt))\n",
    "\n",
    "        mask = ~torch.isin(generated_ids, exclude_ids) #mask to skip the special tokens \n",
    "        generate_len = mask.sum(dim=1)  \n",
    "\n",
    "        with open('./len.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows([[batch[\"id\"][i], batch[\"input_len\"][i], batch[\"target_len\"][i], generate_len[i].item()] for i in range(BATCH_SIZE)])\n",
    "\n",
    "        # Compute ROUGE scores here\n",
    "        rouge_results = rouge.compute(predictions=generated_txt, references=batch[\"highlights\"])\n",
    "        \n",
    "        \n",
    "        with open('./rouge.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([rouge_results['rouge1'], rouge_results['rouge2'], rouge_results['rougeL']])\n",
    "\n",
    "        rouge1_score += rouge_results['rouge1'].item()\n",
    "        rouge2_score += rouge_results['rouge2'].item()\n",
    "        rougeL_score += rouge_results['rougeL'].item()\n",
    "\n",
    "        nb_sample+=1\n",
    "\n",
    "        if nb_sample == 2:\n",
    "            break\n",
    "        \n",
    "\n",
    "with open('./rouge_total.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"Total_rouge1\", \"Total_rouge2\", \"Total_rougeL\"]\n",
    "    writer.writerow(field)\n",
    "    writer.writerow([rouge1_score/nb_sample*100, rouge2_score/nb_sample*100, rougeL_score/nb_sample*100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset.filter(lambda example: example[\"id\"] == \"42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\")\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model & tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Example input\n",
    "input_text = \"The economy is struggling due to inflation.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass (get decoder outputs)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# Shape of decoder output\n",
    "decoder_hidden_states = outputs.decoder_hidden_states[-1]  # Last decoder layer\n",
    "print(decoder_hidden_states.shape)  # (batch_size, sequence_length, 1024)\n",
    "\n",
    "# Fully Connected layer\n",
    "logits = outputs.logits  # (batch_size, sequence_length, vocab_size)\n",
    "print(logits.shape)  # Expected: (batch_size, sequence_length, 50264)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
