{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Bart-CNN score rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  4\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "NUM_LOADER = 4 #depends of the number of thread \n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "MODEL_HUB = \"facebook/bart-large-cnn\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "max_len = 1024\n",
    "\n",
    "\n",
    "BATCH_SIZE =2\n",
    "\n",
    "NUM_BEAM = 5\n",
    "\n",
    "max_len_resume = 200\n",
    "repetition_penalty=2.0\n",
    "length_penalty=1.0\n",
    "early_stopping=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdbf6e977f440c2838ed1edfabed4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0869fd3358e473180fe6e13b653ec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f06b8a30a4aeaa0cb2fcb58cb5140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55af6b860da34f97af17fbede6a7b304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad7d1bf36ac4d45aae5a30085afd6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603bd6c80f4a4798978409232e7f57fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HUB, clean_up_tokenization_spaces=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_HUB)\n",
    "\n",
    "print(model.config.max_position_embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aed24a86fe480080bf9de06f542bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2972b607cee3434b95010877cf6be75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b714a96b39146618292f84563b47cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00e5fee3ba49b9b2e8467fb0e76f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768604c47e249ee9fb60c6f2362ba82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaecdff8f8154f51bf2d2d7643100a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c253d9db417d4840a90480af5d423c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc09d9803ecf4715aba708dacf288f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c597651832a4def9c869886ef6647f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'highlights', 'id'],\n",
      "    num_rows: 28712\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (e.g., CNN/DailyMail)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
    "\n",
    "\n",
    "# Sample 10% of the dataset\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)[\"test\"]\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Load the saved dataset\n",
    "#dataset = load_from_disk('data/cnn_dailymail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f600aec17b46838f89ec7031eee743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/28712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc39d15ecb424e6aa0ae3358f8bef2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/28712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    # Prefix the \"summarize: \" instruction to each article (can be adjusted depending on your task)\n",
    "    batch[\"article\"] = [\"summarize: \" + article for article in batch[\"article\"]]\n",
    "\n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=max_len)\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=max_len)\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=32)# Save the Hugging Face dataset\n",
    "dataset.save_to_disk('data/cnn_dailymail')\n",
    "print(\"Dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'input_mask', 'input_len', 'target_ids', 'target_mask', 'target_len'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shuffle(seed=5).select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['08cf276c9eadb638e0c7fdc83ce0229c8af5d09b', 'a0965f34cb08bd7db5845f8285dc8a9512d3e590'], 'input_ids': tensor([[    0, 18581,  3916,  ...,     1,     1,     1],\n",
      "        [    0, 18581,  3916,  ...,   185,   480,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0, 10567,     8,   479, 50118, 37779,  5460,  4350,    58,  2967,\n",
      "          2863,    49,  1354,     6,  3066, 26743,   479, 50118,  1213,    58,\n",
      "           303,    23, 31103,   271,   459,  2193,   861,    11, 21690,   479,\n",
      "         50118, 40333,   224,     5,   130,   962,     9,  4363,  6154, 24260,\n",
      "           479, 50118,  5873,  4060,   154,   479,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  5341,    35,  7662,    64,  1807,    25,  1246,     9,  4601,\n",
      "             6,   735,   446,  1565,   161,   479, 50118, 20028, 23794,  1519,\n",
      "            13,  9107,  1748,  2398,    31,   145,  3579,     6,   341,    30,\n",
      "           831,   479, 50118, 33382,     6,  1083,   270, 11709,   405,  1506,\n",
      "          5066,  5202,  3623,   447,     7,  1888, 21773,  4755,   479, 50118,\n",
      "           846, 41025,   260,   394, 18148, 14916,    15,    22, 38544,  1745,\n",
      "          3561,   113,  3422,     6,   361,  4799,   479,     2]]), 'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_len': [667, 1108], 'target_len': [47, 68], 'highlights': ['John and .\\nAudrey Cook were discovered alongside their daughter, Maureen .\\nThey were found at Tremarle Home Park in Cornwall .\\nInvestigators say the three died of carbon monoxide .\\npoisoning .', 'NEW: Libya can serve as example of cooperation, White House spokesman says .\\nResolution calls for preventing nuclear weapons from being stolen, used by military .\\nObama, Russian President Dimitry Medvedev working to reduce stockpiles .\\nVenezuelan president Hugo Chavez on \"Larry King Live\" tonight, 9 ET .']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    id = [item['id'] for item in batch]\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    padded_text_ids = pad_sequence(\n",
    "        [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    padded_text_mask = pad_sequence(\n",
    "        [torch.tensor(item['input_mask'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=0)\n",
    "\n",
    "    decoder_input_ids = pad_sequence(\n",
    "        [torch.tensor(item['target_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = pad_sequence(\n",
    "        [torch.tensor(item['target_mask'], \n",
    "                      dtype=torch.long) for item in batch], \n",
    "                      batch_first=True, \n",
    "                      padding_value=0)\n",
    "    \n",
    "    input_len = [item['input_len'] for item in batch]\n",
    "\n",
    "    target_len = [item['target_len'] for item in batch]\n",
    "\n",
    "    highlights = [item['highlights'] for item in batch]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return {\n",
    "        'id':id,\n",
    "        'input_ids':padded_text_ids,\n",
    "        'attention_mask':padded_text_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'target_mask':decoder_attention_mask,\n",
    "        'input_len': input_len ,\n",
    "        'target_len': target_len,\n",
    "        'highlights': highlights\n",
    "    }\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER,\n",
    "    'pin_memory': True  #  Enables faster GPU transfers\n",
    "    }\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "loader = DataLoader(dataset, **params)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7833333333333332, 'rouge2': 0.5833333333333334, 'rougeL': 0.7833333333333332, 'rougeLsum': 0.7833333333333332}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "candidates = [\"Summarization is cool\",\"I love Machine Learning\",\"Good night\"]\n",
    "\n",
    "references = [\"summarization is beneficial and cool\",\"i think i love Machine Learning\",\"Good night everyone!\"]\n",
    "             \n",
    "results = rouge.compute(predictions=candidates, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./rouge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    writer.writerow(field)\n",
    "\n",
    "with open('./len.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"id\", \"input_len\", \"target_len\", \"generate_len\"]\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "total iter: 14356: 1 iter [01:15, 75.20s/ iter]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "rouge1_score, rouge2_score , rougeL_score = 0, 0, 0\n",
    "nb_sample = 0\n",
    "\n",
    "exclude_ids = torch.tensor([0, 1, 2, 3, 50264]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for _, batch in tqdm.tqdm(enumerate(loader, 0),desc=f'total iter: {len(loader)}', unit=\" iter\"):\n",
    "        \n",
    "\n",
    "        generated_ids = model.generate(\n",
    "              input_ids = batch[\"input_ids\"].to(device),\n",
    "              attention_mask = batch[\"attention_mask\"].to(device), \n",
    "              max_length=max_len_resume, \n",
    "              num_beams=NUM_BEAM,\n",
    "              repetition_penalty=repetition_penalty, \n",
    "              length_penalty=length_penalty, \n",
    "              early_stopping=early_stopping\n",
    "              )   \n",
    "        #print(generated_ids)\n",
    "\n",
    "        generated_txt = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        #print(generated_txt)\n",
    "        #print(type(generated_txt))\n",
    "\n",
    "        mask = ~torch.isin(generated_ids, exclude_ids) #mask to skip the special tokens \n",
    "        generate_len = mask.sum(dim=1)  \n",
    "\n",
    "        with open('./len.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows([[batch[\"id\"][i], batch[\"input_len\"][i], batch[\"target_len\"][i], generate_len[i].item()] for i in range(BATCH_SIZE)])\n",
    "\n",
    "        # Compute ROUGE scores here\n",
    "        rouge_results = rouge.compute(predictions=generated_txt, references=batch[\"highlights\"])\n",
    "        \n",
    "        \n",
    "        with open('./rouge.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([rouge_results['rouge1'], rouge_results['rouge2'], rouge_results['rougeL']])\n",
    "\n",
    "        rouge1_score += rouge_results['rouge1'].item()\n",
    "        rouge2_score += rouge_results['rouge2'].item()\n",
    "        rougeL_score += rouge_results['rougeL'].item()\n",
    "\n",
    "        nb_sample+=1\n",
    "\n",
    "        if nb_sample == 2:\n",
    "            break\n",
    "        \n",
    "\n",
    "with open('./rouge_total.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"Total_rouge1\", \"Total_rouge2\", \"Total_rougeL\"]\n",
    "    writer.writerow(field)\n",
    "    writer.writerow([rouge1_score/nb_sample*100, rouge2_score/nb_sample*100, rougeL_score/nb_sample*100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset.filter(lambda example: example[\"id\"] == \"42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\")\n",
    "filtered_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
