{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Bart-CNN score rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the ROUGE metric\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PROCS =  12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_PROCS = os.cpu_count() \n",
    "NUM_LOADER = 8\n",
    "\n",
    "print(\"NUM_PROCS = \" ,NUM_PROCS)\n",
    "\n",
    "MODEL_HUB = \"facebook/bart-large-cnn\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "max_len = 1024\n",
    "\n",
    "\n",
    "BATCH_SIZE =2\n",
    "\n",
    "NUM_BEAM = 5\n",
    "max_len_resume = 200\n",
    "repetition_penalty=2.0\n",
    "length_penalty=1.0\n",
    "early_stopping=True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HUB, clean_up_tokenization_spaces=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_HUB)\n",
    "\n",
    "print(model.config.max_position_embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (e.g., CNN/DailyMail)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split='train')\n",
    "# Load the saved dataset\n",
    "#dataset = load_from_disk('data/cnn_dailymail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03e47b5947a4845b1a2c37581879fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7e67f25b754e518281e9bf53699a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def len_distrib(batch):\n",
    "\n",
    "    len_articles = []\n",
    "    len_highlights = []\n",
    "    \n",
    "    # Prefix the \"summarize: \" instruction to each article (can be adjusted depending on your task)\n",
    "    batch[\"article\"] = [\"summarize: \" + article for article in batch[\"article\"]]\n",
    "\n",
    "    for article, highlight in zip(batch[\"article\"], batch[\"highlights\"]):\n",
    "        len_articles.append(len(tokenizer(article, truncation=False)[\"input_ids\"]))\n",
    "        len_highlights.append(len(tokenizer(highlight, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "\n",
    "    source = tokenizer(batch[\"article\"],truncation=True, max_length=max_len)\n",
    "    resume = tokenizer(batch[\"highlights\"],truncation=True, max_length=max_len)\n",
    "\n",
    "    return {\n",
    "        'input_ids': source['input_ids'], \n",
    "        'input_mask': source['attention_mask'],\n",
    "        'input_len': len_articles,\n",
    "        'target_ids': resume['input_ids'], \n",
    "        'target_mask': resume['attention_mask'],\n",
    "        'target_len': len_highlights\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.map(len_distrib,num_proc=NUM_PROCS,batched=True,batch_size=32)# Save the Hugging Face dataset\n",
    "dataset.save_to_disk('data/cnn_dailymail')\n",
    "print(\"Dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 18581,  3916,  ...,     1,     1,     1],\n",
      "        [    0, 18581,  3916,  ...,  1441,   479,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0, 29345, 10997,   999,  3028,  7312, 20152,  1516,   984,   844,\n",
      "           448, 13016,    25,    37,  4072,   504,   302,   479, 50118, 22138,\n",
      "          2701,   161,    37,    34,   117,   708,     7,   856,  3961,  1334,\n",
      "            39,  1055,   409,   479, 50118, 28243, 20152,    18,  1107,    31,\n",
      "            78,   292, 10997,  3541,    33,    57,   547,    11,  2416,  1391,\n",
      "           479,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    0,   448,  1342,  2368,  4812,  8039,    11,  2561,    32, 15740,\n",
      "            15,     5,    22,  1990, 35095,  1929,   113, 50118, 40145,  5031,\n",
      "          1063,  1594,   397,   161,   144,    32,    89,    25,    10,   898,\n",
      "             9,    22, 40623,   868, 14383, 17130,   113, 50118,  5771,  3480,\n",
      "         10182,  2122,     6,  3186, 32622,    35,    22,   100,   524,     5,\n",
      "           979,     9,     5,   394,   113, 50118, 10350,  1594,   397,   161,\n",
      "             5,   467,    16, 20134,     8,    37,    18,  2190,    13,   464,\n",
      "           479,     2]]), 'target_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_len': [569, 892], 'target_len': [52, 72]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that add padding for each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad the tokenized content\n",
    "    padded_text_ids = pad_sequence(\n",
    "        [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    padded_text_mask = pad_sequence(\n",
    "        [torch.tensor(item['input_mask'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=0)\n",
    "\n",
    "    decoder_input_ids = pad_sequence(\n",
    "        [torch.tensor(item['target_ids'], dtype=torch.long) for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=tokenizer.pad_token_id)     \n",
    "    \n",
    "    decoder_attention_mask = pad_sequence(\n",
    "        [torch.tensor(item['target_mask'], \n",
    "                      dtype=torch.long) for item in batch], \n",
    "                      batch_first=True, \n",
    "                      padding_value=0)\n",
    "    \n",
    "    input_len = [item['input_len'] for item in batch]\n",
    "\n",
    "    target_len = [item['target_len'] for item in batch]\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'input_ids':padded_text_ids,\n",
    "        'attention_mask':padded_text_mask,\n",
    "        'decoder_input_ids':decoder_input_ids,\n",
    "        'target_mask':decoder_attention_mask,\n",
    "        'input_len': input_len ,\n",
    "        'target_len': target_len\n",
    "    }\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'collate_fn':collate_fn,\n",
    "    'num_workers': NUM_LOADER\n",
    "    }\n",
    "\n",
    "# This will be used down for training and validation stage for the model.\n",
    "loader = DataLoader(dataset, **params)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.7833333333333332, 'rouge2': 0.5833333333333334, 'rougeL': 0.7833333333333332, 'rougeLsum': 0.7833333333333332}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "candidates = [\"Summarization is cool\",\"I love Machine Learning\",\"Good night\"]\n",
    "\n",
    "references = [\"Summarization is beneficial and cool\",\"I think i love Machine Learning\",\"Good night everyone!\"]\n",
    "             \n",
    "results = rouge.compute(predictions=candidates, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./rouge.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    writer.writerow(field)\n",
    "\n",
    "with open('./len.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"input_len\",\"target_len\", \"generate_len\"]\n",
    "    writer.writerow(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "total iter: 143557: 2 iter [00:30, 15.21s/ iter]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "rouge1_score, rouge2_score , rougeL_score = 0, 0, 0\n",
    "nb_sample = 0\n",
    "\n",
    "exclude_ids = torch.tensor([0, 1, 2, 3, 50264]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for _, batch in tqdm.tqdm(enumerate(loader, 0),desc=f'total iter: {len(loader)}', unit=\" iter\"):\n",
    "        \n",
    "        y_txt = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "              input_ids = batch[\"input_ids\"].to(device),\n",
    "              attention_mask = batch[\"attention_mask\"].to(device), \n",
    "              max_length=max_len_resume, \n",
    "              num_beams=NUM_BEAM,\n",
    "              repetition_penalty=repetition_penalty, \n",
    "              length_penalty=length_penalty, \n",
    "              early_stopping=early_stopping\n",
    "              )   \n",
    "        #print(generated_ids)\n",
    "\n",
    "        generated_txt = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        #print(generated_txt)\n",
    "\n",
    "        mask = ~torch.isin(generated_ids, exclude_ids)  \n",
    "        generate_len = mask.sum(dim=1)  \n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            with open('./len.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([batch[\"input_len\"][i],batch[\"target_len\"][i],generate_len[i].item()])\n",
    "\n",
    "        # Compute ROUGE scores here\n",
    "        rouge_results = rouge.compute(predictions=generated_txt, references=y_txt)\n",
    "        \n",
    "        \n",
    "        with open('./rouge.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([rouge_results['rouge1'], rouge_results['rouge2'], rouge_results['rougeL']])\n",
    "\n",
    "        rouge1_score += rouge_results['rouge1']\n",
    "        rouge2_score += rouge_results['rouge2']\n",
    "        rougeL_score += rouge_results['rougeL']\n",
    "\n",
    "        nb_sample+=1\n",
    "\n",
    "        if nb_sample == 3:\n",
    "            break\n",
    "        \n",
    "\n",
    "with open('./rouge_total.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    field = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "    writer.writerow(field)\n",
    "    writer.writerow([rouge1_score/nb_sample*100, rouge2_score/nb_sample*100, rougeL_score/nb_sample*100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a999caeb9461f92c45b2a521d9042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'input_mask', 'input_len', 'target_ids', 'target_mask', 'target_len'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda example: example[\"id\"] == \"42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\")\n",
    "filtered_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
